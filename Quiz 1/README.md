Topics for Quiz 1 (based on materials of Week 1-2)

Week 1 Lecture presentation

- W1: Definition of machine learning
- W1: When to use / not to use machine learning
- W1: Main fields of machine learning (Supervised, unsupervised, reinforcement)
- W1: Regression task in machine learning
- W1: Classification task in machine learning
- W1: Clustering task in machine learning
- W1: Difference between parameters and hyper-parameters
- W1: Dataset splitting.

Week 1 Practice presentation

- W1: Difference between Lazy and Eager learning algorithms
- W1: k-Nearest Neighbors - definition and classification (in context of lazy/eager and parametric/non-parametric)
- W1: Main hyper-parameters of kNN: k, distance metric
- W1: Fit-predict pipeline of kNN
- W1: How to find best value for "k" hyper-parameter for kNN?
- W1: Which metric is more suitable for sparse data? Which metric measures the physical distance between points?
- W1: Limitations of kNN algorithm

Week 1 Practice presentation

- W1: k-Means - definition and classification (in context of lazy/eager and parametric/non-parametric)
- W1: Main hyper-parameters of k-Means: k, distance metric, max_iters
- W1: Centroids in k-Means algorithm
- W1: Fit-predict pipeline of k-Means
- W1: Main problem of random initialization of centroids
- W1: How to find the best number of centroids?

Week 2 Practice presentation

- W2: Decision Tree Algorithm - definition and classification (in context of lazy/eager and parametric/non-parametric)
- W2: Independent and dependent attributes
- W2: Categorical and numerical attributes
- W2: Building blocks of Decision Tree Algorithm (node, edge, leaf)

Week 2 Video (at the end of presentation)

- W2: Gini-index
- W2: How to choose best attribute for splitting at node? (based on Gini-index)

Week 2 Colab notebook

- W2: "max_depth" hyper-parameter in decision tree algorithm
- W2: How to find the best value for max_depth hyper-parameter?
- W2: Overfitting of the model and regularization
- W2: How regularization of max_depth hyper-parameter (and others) helps to prevent overfitting?
- W2: What is the difference between perfectly fitting ML algorithm on train data and just finding common pattern in train data to make predictions? Why second one commonly works well on validation/test sets or real data in compare to the first one?
